{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLxZ3xcWzqA7",
    "outputId": "71273fc6-6fa9-4a9b-e50d-9d3cc61dc8f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/liviaclarete/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, words\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download stopwords and English words from NLTK if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDxtnk4KLzb1"
   },
   "source": [
    "# Loading and Preprocessing data\n",
    "\n",
    "Output: tokens/POS counts by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "VvvcD54Ny_Db"
   },
   "outputs": [],
   "source": [
    "def get_ef_data_by_volume_id(volume_id):\n",
    "    \"\"\"\"Fetches the extract featureset data for a given volume\"\"\"\n",
    "    url = f\"{BASE_URL_HTRC}/volumes/{volume_id}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()[\"data\"][\"features\"][\"pages\"]\n",
    "\n",
    "\n",
    "def get_ht_bib_metadata(id_type, id_value):\n",
    "    \"\"\"Fetches the volume metadata for a given standard identifier.\n",
    "    id_type should be one of oclc, issn, isbn, issn, htid, recordnumber\"\"\"\n",
    "    url = f\"{BASE_URL_HT}/{id_type}/{id_value}.json\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def extract_ef_ids(lawrence_metadata):\n",
    "    \"\"\"\n",
    "    Extracts specific fields from items in lawrence_metadata.\n",
    "\n",
    "    Parameters:\n",
    "    lawrence_metadata (dict): A dictionary containing metadata with a key \"items\"\n",
    "                              which is a list of dictionaries.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing \"orig\", \"htid\", and \"enumcron\" fields.\n",
    "    \"\"\"\n",
    "    ef_ids = []\n",
    "    for item in lawrence_metadata[\"items\"]:\n",
    "        ef_item = {\n",
    "            \"orig\": item[\"orig\"],\n",
    "            \"htid\": item[\"htid\"],\n",
    "            \"enumcron\": item[\"enumcron\"]\n",
    "        }\n",
    "        ef_ids.append(ef_item)\n",
    "\n",
    "#   Two possible edge cases:\n",
    "# - Multiple volumes in a multivolume work (with a single OCLC)\n",
    "# - Multiple HathiTrust volumes = single-volume work (single OCLC)\n",
    "    # if len(ef_ids) > 1:\n",
    "    return ef_ids[0]['htid']\n",
    "    # return ef_ids\n",
    "\n",
    "# Define stopwords and English vocabulary\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "def extract_data_from_pages(pages):\n",
    "    \"\"\"Extract token data from pages, applying preprocessing steps.\"\"\"\n",
    "    extracted_data = []\n",
    "    for i, page in enumerate(pages):\n",
    "        body = page.get('body')\n",
    "        if body:\n",
    "            token_pos_count = body.get('tokenPosCount')\n",
    "            if token_pos_count:\n",
    "                for t, pos in token_pos_count.items():\n",
    "                    t = t.lower()\n",
    "                    if t not in stop_words and re.match(\"^[a-zA-Z]+$\", t) and t in english_vocab:\n",
    "                        token_data = {\"page\": i, \"token\": t}\n",
    "                        pos_dict = dict(zip([\"pos\", \"counts\"], list(pos.items())[0]))\n",
    "                        token_data.update(pos_dict)\n",
    "                        extracted_data.append(token_data)\n",
    "    return extracted_data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_book_data(data, csv_filename='book_data.csv'):\n",
    "    \"\"\"\n",
    "    Extracts book data from the given data structure and creates a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - data (dict): The data structure containing book information.\n",
    "    - csv_filename (str): The name of the CSV file to save the DataFrame to.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with columns 'titles', 'oclcs', 'publishDates', 'orig', 'itemURL'.\n",
    "    \"\"\"\n",
    "    records = data['records']\n",
    "    items = data['items']\n",
    "    \n",
    "    # Initialize lists to store the extracted data\n",
    "    titles = []\n",
    "    oclcs = []\n",
    "    publish_dates = []\n",
    "    orig = []\n",
    "    item_urls = []\n",
    "    \n",
    "    # Extract data from records\n",
    "    for record_id, record_info in records.items():\n",
    "        titles.append(record_info['titles'][0] if record_info['titles'] else None)\n",
    "        oclcs.append(record_info['oclcs'][0] if record_info['oclcs'] else None)\n",
    "        publish_dates.append(record_info['publishDates'][0] if record_info['publishDates'] else None)\n",
    "    \n",
    "    # Extract data from items\n",
    "    for item in items:\n",
    "        orig.append(item['orig'])\n",
    "        item_urls.append(item['itemURL'])\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'titles': titles,\n",
    "        'oclcs': oclcs,\n",
    "        'publishDates': publish_dates,\n",
    "        'orig': orig,\n",
    "        'itemURL': item_urls\n",
    "    })\n",
    "    \n",
    "    # Transpose the DataFrame and reset the index\n",
    "    df = df.T\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file, excluding the first line\n",
    "    df.to_csv(csv_filename, index=False, header=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main(oclc):\n",
    "    metadata = get_ht_bib_metadata(\"oclc\", oclc)\n",
    "    extract_book_data(metadata, 'book_data.csv')\n",
    "    mich = get_ef_data_by_volume_id(extract_ef_ids(metadata))\n",
    "    ef_mich = extract_data_from_pages(mich)\n",
    "    df = pd.DataFrame.from_records(ef_mich).sort_values(by='counts', ascending=False)\n",
    "    df['cumulative_sum'] = df.groupby('token')['counts'].cumsum()\n",
    "    df['length'] = df['token'].str.len()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "mRCB-l0q-DdF"
   },
   "outputs": [],
   "source": [
    "BASE_URL_HTRC = \"https://data.htrc.illinois.edu/ef-api\"\n",
    "BASE_URL_HT = \"https://catalog.hathitrust.org/api/volumes/full/\"\n",
    "\n",
    "oclc_lawrence = \"3580950\"\n",
    "df = main(oclc_lawrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b82GQudHMP-b"
   },
   "source": [
    "# Basic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_top_20_frequent_words(df, token_column='token', pdf=None, fig_size=(15, 6)):\n",
    "    # Count the frequency of each token directly from the DataFrame\n",
    "    top_20_freq = df[token_column].value_counts()[:20]\n",
    "\n",
    "    # Create a DataFrame with the top 20 most frequent words\n",
    "    top20 = pd.DataFrame()\n",
    "    top20['Words'] = list(top_20_freq.index)\n",
    "    top20['Counts'] = list(top_20_freq.values)\n",
    "\n",
    "    # Generate a color palette\n",
    "    colors = sns.color_palette(\"viridis\", len(top20))\n",
    "\n",
    "    # Plot the results using seaborn\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    bars = sns.barplot(x='Words', y='Counts', data=top20, palette=colors, ax=ax)\n",
    "    bars.set(title='Top 20 Most Frequent Words in the Corpus')\n",
    "\n",
    "    if pdf:\n",
    "        pdf.savefig(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def generate_word_cloud(df, token_column='token', count_column='counts', pdf=None, fig_size=(10, 6)):\n",
    "    # Create a dictionary of token counts\n",
    "    token_counts = df.groupby(token_column)[count_column].sum().to_dict()\n",
    "\n",
    "    # Create a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(token_counts)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Word Cloud of Most Frequent Tokens')\n",
    "\n",
    "    if pdf:\n",
    "        pdf.savefig(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_token_length_distribution(df, token_column='token', pdf=None, fig_size=(18, 6)):\n",
    "    # Add a column for token lengths\n",
    "    df['token_length'] = df[token_column].apply(len)\n",
    "\n",
    "    # Plot the distribution of token lengths\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    sns.histplot(df['token_length'], bins=15, kde=True, ax=ax)\n",
    "    ax.set_title('Distribution of Token Lengths')\n",
    "    ax.set_xlabel('Token Length')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    if pdf:\n",
    "        pdf.savefig(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_top_words_evolution(df, token_column='token', count_column='counts', page_column='page', top_n=10, pdf=None, fig_size=(10, 6)):\n",
    "    # Sorting to find the top N words by counts\n",
    "    top_words = df.groupby(token_column)[count_column].sum().sort_values(ascending=False).head(top_n).index\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    for word in top_words:\n",
    "        word_data = df[df[token_column] == word]\n",
    "        ax.scatter(word_data[page_column], [word] * len(word_data), s=word_data[count_column] * 10, label=word, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel('Page')\n",
    "    ax.set_ylabel('Words')\n",
    "    ax.set_title('Evolution of Top Words by Page')\n",
    "    ax.legend(title='Words', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "    ax.grid(True)\n",
    "\n",
    "    if pdf:\n",
    "        pdf.savefig(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def export_charts_to_pdf(main_df, book_csv_path, pdf_path, fig_size=(10, 6)):\n",
    "    \"\"\"\n",
    "    Exports several charts into a single PDF document with book data on the first page.\n",
    "\n",
    "    Args:\n",
    "    - main_df (pd.DataFrame): The DataFrame containing the data for the charts.\n",
    "    - book_csv_path (str): The file path to the CSV file containing book data.\n",
    "    - pdf_path (str): The file path to save the PDF document.\n",
    "    - fig_size (tuple): The figure size for all charts.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Read the book data from CSV\n",
    "    book_data = pd.read_csv(book_csv_path)\n",
    "    \n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        # Create a figure for the book data\n",
    "        fig, ax = plt.subplots(figsize=(18, 6))\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Book Data')\n",
    "        \n",
    "        # Create a table from the book data and add it to the plot\n",
    "        table = ax.table(cellText=book_data.values, colLabels=book_data.columns, cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.2)\n",
    "\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Plot the charts\n",
    "        plot_top_20_frequent_words(main_df, pdf=pdf, fig_size=(18, 6))\n",
    "        generate_word_cloud(main_df, pdf=pdf, fig_size=(18, 6))\n",
    "#         plot_token_length_distribution(main_df, pdf=pdf, fig_size=fig_size)\n",
    "        plot_top_words_evolution(main_df, pdf=pdf, fig_size=(18, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_charts_to_pdf(df, 'book_data.csv', 'charts_output.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import mpld3\n",
    "\n",
    "def plot_top_20_frequent_words(df, token_column='token', fig_size=(15, 6)):\n",
    "    if token_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{token_column}' not found in DataFrame\")\n",
    "\n",
    "    # Count the frequency of each token directly from the DataFrame\n",
    "    top_20_freq = df[token_column].value_counts()[:20]\n",
    "\n",
    "    # Create a DataFrame with the top 20 most frequent words\n",
    "    top20 = pd.DataFrame()\n",
    "    top20['Words'] = list(top_20_freq.index)\n",
    "    top20['Counts'] = list(top_20_freq.values)\n",
    "\n",
    "    # Generate a color palette\n",
    "    colors = sns.color_palette(\"viridis\", len(top20))\n",
    "\n",
    "    # Plot the results using seaborn\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    bars = sns.barplot(x='Words', y='Counts', data=top20, palette=colors, ax=ax)\n",
    "    bars.set(title='Top 20 Most Frequent Words in the Corpus')\n",
    "\n",
    "    html_str = mpld3.fig_to_html(fig)\n",
    "    plt.close(fig)\n",
    "    return html_str\n",
    "\n",
    "def generate_word_cloud(df, token_column='token', count_column='counts', fig_size=(10, 6)):\n",
    "    if token_column not in df.columns or count_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{token_column}' or '{count_column}' not found in DataFrame\")\n",
    "\n",
    "    # Create a dictionary of token counts\n",
    "    token_counts = df.groupby(token_column)[count_column].sum().to_dict()\n",
    "\n",
    "    # Create a word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(token_counts)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Word Cloud of Most Frequent Tokens')\n",
    "\n",
    "    html_str = mpld3.fig_to_html(fig)\n",
    "    plt.close(fig)\n",
    "    return html_str\n",
    "\n",
    "def plot_token_length_distribution(df, token_column='token', fig_size=(18, 6)):\n",
    "    if token_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{token_column}' not found in DataFrame\")\n",
    "\n",
    "    # Add a column for token lengths\n",
    "    df['token_length'] = df[token_column].apply(len)\n",
    "\n",
    "    # Plot the distribution of token lengths\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    sns.histplot(df['token_length'], bins=15, kde=True, ax=ax)\n",
    "    ax.set_title('Distribution of Token Lengths')\n",
    "    ax.set_xlabel('Token Length')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "    html_str = mpld3.fig_to_html(fig)\n",
    "    plt.close(fig)\n",
    "    return html_str\n",
    "\n",
    "def plot_top_words_evolution(df, token_column='token', count_column='counts', page_column='page', top_n=10, fig_size=(10, 6)):\n",
    "    if token_column not in df.columns or count_column not in df.columns or page_column not in df.columns:\n",
    "        raise KeyError(f\"Column '{token_column}', '{count_column}', or '{page_column}' not found in DataFrame\")\n",
    "\n",
    "    # Sorting to find the top N words by counts\n",
    "    top_words = df.groupby(token_column)[count_column].sum().sort_values(ascending=False).head(top_n).index\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    for word in top_words:\n",
    "        word_data = df[df[token_column] == word]\n",
    "        ax.scatter(word_data[page_column], [word] * len(word_data), s=word_data[count_column] * 10, label=word, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel('Page')\n",
    "    ax.set_ylabel('Words')\n",
    "    ax.set_title('Evolution of Top Words by Page')\n",
    "    ax.legend(title='Words', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n",
    "    ax.grid(True)\n",
    "\n",
    "    html_str = mpld3.fig_to_html(fig)\n",
    "    plt.close(fig)\n",
    "    return html_str\n",
    "\n",
    "df['length'].plot(kind='box', vert=False, figsize=(8, 1))\n",
    "df['length'].plot(kind='hist', bins=30, figsize=(8,2))\n",
    "\n",
    "def export_charts_to_html(main_df, html_path, fig_size=(10, 6)):\n",
    "    \"\"\"\n",
    "    Exports several charts into a single HTML document.\n",
    "\n",
    "    Args:\n",
    "    - main_df (pd.DataFrame): The DataFrame containing the data for the charts.\n",
    "    - html_path (str): The file path to save the HTML document.\n",
    "    - fig_size (tuple): The figure size for all charts.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <head><title>Charts</title></head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "\n",
    "    # Add each plot to the HTML content\n",
    "    try:\n",
    "        html_content += plot_top_20_frequent_words(main_df, fig_size=(18, 6))\n",
    "    except KeyError as e:\n",
    "        html_content += f\"<p>{str(e)}</p>\"\n",
    "\n",
    "    try:\n",
    "        html_content += generate_word_cloud(main_df, fig_size=(18, 6))\n",
    "    except KeyError as e:\n",
    "        html_content += f\"<p>{str(e)}</p>\"\n",
    "\n",
    "    try:\n",
    "        html_content += plot_token_length_distribution(main_df, fig_size=fig_size)\n",
    "    except KeyError as e:\n",
    "        html_content += f\"<p>{str(e)}</p>\"\n",
    "\n",
    "    try:\n",
    "        html_content += plot_top_words_evolution(main_df, fig_size=(18, 6))\n",
    "    except KeyError as e:\n",
    "        html_content += f\"<p>{str(e)}</p>\"\n",
    "\n",
    "    # Close the HTML tags\n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Write the HTML content to a file\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "        \n",
    "# Example usage:\n",
    "# df = pd.read_csv('book_data.csv')  # Replace with your actual data\n",
    "export_charts_to_html(df, 'results.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
